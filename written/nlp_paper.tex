% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath} % Added for \tfrac and other math commands
\usepackage{amssymb} % Added for \mathbb and additional math symbols

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SampledKD: Extreme Sampling, Full Distillation Accuracy.}
% \title{SampledKD: How Extreme Can Sampling Be While Matching Accuracy?}
% \title{SampledKD: How Efficient Could Distillation Get While Match Accuracy?}
% \title{SampledKD: Could Distillation be So Much More Efficient?}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Neil Cnaan\textsuperscript{*} \qquad Itay Ebenspanger\textsuperscript{*} \qquad Almog Tavor\textsuperscript{*} \\
  Balvatnik School of Computer Science and AI \\
  Tel Aviv University \\
  \texttt{\{email1, email2, email3\}@mail.tau.ac.il} \\
}

\begin{document}
\maketitle
\let\thefootnote\relax
\footnotemark
\footnotetext{* Equal contribution.}
\begin{abstract}
	Large language models (LLMs) achieve impressive results but are challenging to deploy due to their size and computational demands. Knowledge distillation (KD) is a popular approach for compressing LLMs, yet existing methods often suffer from efficiency-accuracy trade-offs, especially when applied to autoregressive models. We propose Entropy Knowledge Distillation (EKD), a token-selective distillation framework that leverages teacher entropy and other uncertainty measures to focus supervision on informative tokens, while maintaining unbiased class-level guidance. EKD enables efficient student training by reducing the need to store or query full teacher logits, and improves calibration and performance compared to top-$K$ truncation and other sparse KD methods. Experiments on standard language modeling benchmarks demonstrate that EKD achieves superior compression-accuracy trade-offs, paving the way for more practical deployment of compact LLMs.
\end{abstract}

\section{Introduction}

Large language models (LLMs) deliver state-of-the-art performance but are costly to serve and adapt. Knowledge distillation (KD) amortizes these costs by training a compact student to imitate a larger teacher \citep{hinton2015distillation}. While classic logit-based KD is effective for encoder-style models (e.g., DistilBERT \citep{sanh2019distilbert}), applying KD to autoregressive LLMs raises two persistent obstacles: (i) \textbf{train-inference mismatch} in sequence generation, and (ii) \textbf{efficiency limits} from storing or querying full teacher logits.

To address (i), recent work proposes \emph{on-policy} distillation that trains the student on prefixes it actually produces, with the teacher scoring those prefixes \citep{agarwal2024gkd}. To address (ii), sparse alternatives avoid caching the full distribution. However, deterministic top-$K$ truncation of teacher logits (e.g., keeping only the largest $K$ probabilities) yields biased supervision and degraded calibration, especially for small $K$ \citep{anshumann2025sparse,shum2024first}. Storing more logits improves quality but erodes the efficiency goal.

We revisit \emph{where} and \emph{how} to apply teacher supervision for LLMs. Our thesis is that the KD budget should be spent \emph{selectively at the token level} (where the teacher can add the most information) while keeping \emph{unbiased} class-level supervision when it is applied. Concretely, we:

\begin{itemize}
	\item Introduce \textbf{token-selective distillation}: choose a subset of tokens for KD using scores such as teacher entropy, teacher-student KL, and student CE;
	\item Estimate the teacher distribution at selected tokens with \textbf{Random-Sampling KD} - importance-sampling teacher classes and reweighting—so the gradient matches full KD in expectation while storing only a handful of logits \citep{anshumann2025sparse};
	\item Maintain standard cross-entropy (CE) on \emph{all} tokens to stabilize training and calibration \citep{guo2017calibration}; and
	\item Add simple curricula (increasing KD coverage over time) and optional EMA-based self-distillation for robustness.
\end{itemize}

Empirically, we compare against (a) full Random-Sampling KD at every token, (b) deterministic top-$K$ logit truncation baselines including SLIM-style sparse logits \citep{raman2023slim}, and (c) token/data selection schemes such as GLS for NMT \citep{wang2021selectivekd}, token-level uncertainty-aware post-training \citep{liu2025tokenlevel}, and progressive chain-of-thought distillation \citep{feng2024kpod}. We evaluate both in-distribution and under distribution shift using the ShiftKD protocol \citep{zhang2023shiftkd}. Our results indicate that token-selective, \emph{unbiased} KD attains a stronger accuracy-efficiency Pareto frontier while preserving calibration.

\subsection{Contributions} (1) A unified framework for \emph{token-level} selection with \emph{unbiased} class-level distillation; (2) practical scoring, curriculum, and bandit variants for allocating KD budget; (3) comprehensive evaluation vs. state-of-the-art sparse KD and selection baselines, including calibration and shift robustness.

\section{Entropy Approximation}
\label{sec:entropy}
We estimate token uncertainty via a \emph{Top-$k$-plus-tail} construction. Let $u \in \mathbb{R}^V$ be teacher logits over a vocabulary of size $V$, and let
\[
	p_i = \frac{e^{u_i}}{Z},\qquad Z = \sum_{j=1}^{V} e^{u_j}
\]
denote the softmax probabilities and partition function. Order indices so that $u_{(1)} \geq \cdots \geq u_{(m)}$ are the top-$m$ logits. Define the top-$m$ log-partition and an \emph{upper bound} on the full log-partition by
\begin{align*}
	Z_m \;           & =\; \log \sum_{j \leq m} e^{u_{(j)}}\,,         \\
	Z_{\text{ub}} \; & =\; \log\Big(e^{Z_m} + (V-m)\,e^{u_{(m)}}\Big).
\end{align*}
The bound $Z_{\text{ub}}$ follows by pessimistically setting all tail logits to the $m$-th largest value $u_{(m)}$. Consequently, for $j \leq m$ we obtain \emph{conservative} (lower) probabilities
\[
	p^{\min}_{(j)} = \exp\big(u_{(j)} - Z_{\text{ub}}\big),\quad
	\tau_{\max} = 1 - \sum_{j \leq m} p^{\min}_{(j)}\,.
\]
Here $\tau_{\max}$ upper-bounds the probability mass outside the top-$m$ (the \emph{tail}). A valid \textbf{lower bound} on the entropy $H(p) = -\sum_i p_i \log p_i$ is then
\[
	H_{\text{lb}} = -\sum_{j \leq m} p^{\min}_{(j)} \log p^{\min}_{(j)} \; - \; \tau_{\max} \log \tau_{\max}\,.
\]
This follows from entropy's monotonicity under coarse-graining: the smallest entropy consistent with the constraints is achieved by merging all tail mass into a single bin.
One may also define an \emph{upper} estimate $H_{\text{ub}} = H_{\text{lb}} + \tau_{\max}\log(V-m)$ and a midpoint $\widehat{H}_m = \tfrac{1}{2}(H_{\text{lb}}+H_{\text{ub}})$. We rank tokens using $H_{\text{lb}}$ and use TkT-LB with $m{=}20$ for a strong efficiency-accuracy tradeoff.

This cuts computation and storage from $\mathcal{O}(V)$ to $\mathcal{O}(m)$: with a 30K vocab and $m{=}20$, we process $\sim20$ terms (plus one tail) instead of 30{,}000—about $1{,}500\times$ fewer—and keep only 20 logits, while remaining a strong proxy for entropy ranking.

\section{Position-Selective Distillation (RS-KD over Positions)}
\label{sec:posrs}

The KD objective for an autoregressive step is a mean over positions:
\[
	\mathcal{L}_{\text{KD}} \;=\; \frac{1}{N_{\text{pos}}} \sum_{t=1}^{N_{\text{pos}}}
	\mathrm{KL}\!\left(p_t \,\|\, s_t\right),
\]
where $p_t$ and $s_t$ are teacher and student class distributions at position $t$.
Full KD evaluates all positions; we instead \emph{sample positions} using a proposal
$q_{\text{pos}}(t)$ to focus compute on informative steps while keeping the estimate (approximately) unbiased.

\subsection{Sampling rule}
We form $q_{\text{pos}}(t) \propto H_t^\alpha$ from a per-token uncertainty score $H_t$
(e.g., the TkT-LB lower-bound entropy from \S\ref{sec:entropy}, with $m{=}20$) and exponent $\alpha\!\ge\!0$ (higher $\alpha$ concentrates on uncertain tokens). For a sample size $K_{\text{pos}}$, the with-replacement importance-sampling estimator is
\[
	\widehat{\mathcal{L}}_{\text{KD}}^{\text{pos}} \;=\;
	\frac{1}{K_{\text{pos}}} \sum_{t \sim q_{\text{pos}}}
	\frac{1}{q_{\text{pos}}(t)} \,\mathrm{KL}\!\left(p_t \,\|\, s_t\right),
\]
which is unbiased for the uniform positional mean. In practice, we sample \emph{without} replacement within each sequence for lower variance and use a Hájek-style normalization: $w_t \propto 1/q_{\text{pos}}(t)$, $\sum_{t\in S} w_t=1$, then take $\sum_{t\in S} w_t\,\mathrm{KL}(p_t\|s_t)$.
A tiny probability floor (e.g., $10^{-6}$) stabilizes $1/q$.

\subsection{Discussion}
This \emph{position-selective} KD is orthogonal to RS-KD over classes: it reduces compute along the \emph{time/sequence} axis, not the vocabulary axis. It complements RS-KD (next section) and preserves coverage: low-uncertainty positions still contribute with nonzero probability (controlled by $\alpha$).
We keep CE on all positions for stability, and treat the position-sampled KD as an additive term.

\vspace{0.5em}
\noindent\textbf{Defaults.}
$\alpha{=}1$, $K_{\text{pos}}\!\in\![8,32]$, weight floor $10^{-6}$; optionally anneal $\alpha$ upward during training to shift from exploration to focused supervision.


\section{Two-Axis Offline Sampling with Entropy Proxies (SampledKD)}
\label{sec:twoaxis}

We combine (i) \emph{position-selective} KD (Y axis) with (ii) \emph{RS-KD over classes} (X axis) and (iii) a lightweight \emph{entropy approximation} to build an efficient, unbiased estimator we call \textbf{SampledKD}.
Crucially, we separate \emph{offline} teacher processing from \emph{online} student training.

\subsection{Offline stage (teacher, once)}
For each training sequence and position $t$:
\begin{enumerate}
	\item Compute a top-$m$ entropy proxy $H_t$ via TkT-LB (\S\ref{sec:entropy}) to drive position selection.
	\item Construct a class proposal $q_{\text{voc}}(\cdot\,|\,t)$ (typically $q_{\text{voc}}\!=\!p_t$, or a tempered variant) and draw $R$ samples with replacement. Let $C_t(i)$ be the sampled count of class $i$.
	\item Form a sub-distribution over the sampled classes using importance weights
	      \[
		      \tilde p_t(i) \;\propto\; \frac{p_t(i)}{q_{\text{voc}}(i\,|\,t)} \, C_t(i),
		      \qquad i \in \mathcal{S}_t,
	      \]
	      then normalize over $\mathcal{S}_t=\{i: C_t(i)>0\}$. Choosing $R$ so that $\mathbb{E}[|\mathcal{S}_t|]\!\approx\!K_{\text{voc}}$ yields a budget comparable to top-$K_{\text{voc}}$ but without truncation bias.
\end{enumerate}
Only $(\mathcal{S}_t, \tilde p_t)$ and the scalar $H_t$ are stored (no full logits), typically in fp16.

\subsection{Online stage (student, many epochs)}
At each batch we:
\begin{enumerate}
	\item For each sequence, sample a subset $S$ of positions using $q_{\text{pos}}(t)\!\propto\!H_t^\alpha$ (without replacement, $|S|{=}K_{\text{pos}}$) and weights $w_t \propto 1/q_{\text{pos}}(t)$, $\sum_{t\in S}w_t{=}1$.
	\item For each $t \in S$, compute the \emph{sampled} KD term only on the cached subset:
	      \begin{align*}
		      \mathrm{KL}\!\left(\tilde p_t \,\middle\|\, s_t(\cdot\,;\,\mathcal{S}_t)\right)
		      \; & =\; \sum_{i \in \mathcal{S}_t} \tilde p_t(i)\,\log \tilde p_t(i) \\
		         & \quad - \sum_{i \in \mathcal{S}_t} \tilde p_t(i)\,\log s_t(i).
	      \end{align*}
	\item Aggregate over positions with the positional weights and add CE on all tokens:
	      \begin{align*}
		      \mathcal{L} \; & =\; \sum_{t\in S} w_t\, \mathrm{KL}\!\left(\tilde p_t \,\|\, s_t\right) \\
		                     & \quad + \alpha_{\text{ce}} \cdot \text{CE}(\text{gold}, s).
	      \end{align*}
\end{enumerate}

\subsection{Why it works}
RS-KD's importance weighting ensures that, for fixed $t$, the expectation of the class-sampled KD equals full KD at $t$; positional importance weighting targets the uniform mean over positions.
TkT-LB supplies $H_t$ at $\mathcal{O}(m)$ cost, so both sampling decisions avoid any full-$V$ passes.
Together, SampledKD reduces cost from $\mathcal{O}(N_{\text{pos}}V)$ to $\mathcal{O}(K_{\text{pos}}K_{\text{voc}})$ per sequence while matching full-KD gradients in expectation.

\subsection{Complexity and knobs}
Storage per position is $\mathcal{O}(K_{\text{voc}})$ (indices $+$ probs). Online compute scales with $K_{\text{pos}}\!\times\!K_{\text{voc}}$. Good defaults:
$K_{\text{voc}}\!\approx\!12$ expected uniques (choose $R$ accordingly),
$\tau{=}1$ for $q_{\text{voc}}$ temperature,
$K_{\text{pos}}\!\in\![8,32]$,
$\alpha{=}1$,
$\alpha_{\text{ce}}\!\in\![0.05,0.2]$.
A tiny probability floor prevents numerical blow-ups in $1/q$.


\section{Related Work}

\subsection{KD for LLMs} Foundational KD \citep{hinton2015distillation} and encoder-model distillation (e.g., DistilBERT \citep{sanh2019distilbert}) have been extended to generative LMs: sequence-level KD for NMT \citep{kim2016sequencekd}, reverse-KL objectives for generation (MiniLLM) \citep{gu2023minillm}, and on-policy distillation (GKD) to mitigate exposure bias \citep{agarwal2024gkd}. Cross-tokenizer distillation has been addressed by Universal Logit Distillation (ULD) via optimal transport \citep{boizard2024uld}.

\subsection{Sparse logit KD and calibration} Deterministic top-$K$/percentile caching of teacher logits (e.g., SLIM \citep{raman2023slim} and top-$5$ variants) reduces storage but discards tail mass, inducing biased gradients and miscalibrated students. Random-Sampling KD \citep{anshumann2025sparse} replaces truncation with importance sampling to provide \emph{unbiased} estimates that match full-KD gradients in expectation with minimal overhead. Trustworthy distillation explicitly studies calibration and proposes processing the top-$k$ teacher tokens to reduce miscalibration \citep{shum2024first}. We measure calibration via Expected Calibration Error (ECE) \citep{guo2017calibration}.

\subsection{Token-/word-level selective supervision} Several strands selectively apply supervision at the granularity we target. In NMT, \citet{wang2021selectivekd} select high cross-entropy \emph{words} using both batch-local and global FIFO queues (GLS). For LLM post-training, token-level uncertainty-aware learning applies masked MLE on high-uncertainty \emph{tokens} and self-distillation on the remainder to avoid OOD overfitting \citep{liu2025tokenlevel}. In continual KD, token-level cross-entropy is used to quantify when and where to distill \citep{zhang2023continualkd}. In CoT reasoning, KPOD learns token importance weights and a progressive schedule within rationales \citep{feng2024kpod}. Compared to these, our EKD selects tokens using teacher-centric uncertainty (entropy and teacher-student KL) and couples selection with \emph{unbiased} class-level Random-Sampling KD at selected positions; we also keep CE on all tokens for stability.

\subsection{Entropy/uncertainty-guided KD} Beyond selection, multiple KD variants weight supervision by uncertainty or entropy. Entropy-based adaptive KD (EA-KD) prioritizes \emph{samples} with higher entropy \citep{su2023eakd}, while DE-MKD uses teacher-prediction entropy to weight among multiple teachers \citep{cheng2024demkd}. Uncertainty-aware mixup reduces KD cost while maintaining quality \citep{xu2023unix}. Recent work also shows entropy-weighted distillation can improve reliability/calibration in classification \citep{guo2024entropykd}. Our EKD differs in using entropy/KL primarily to decide \emph{which tokens} receive external-teacher KD, not to reweight losses globally.

% (moved) Entropy approximation subsection relocated before Related Work.

\subsection{Entropy/perplexity for data selection} Selecting \emph{samples} by entropy or perplexity is long-standing in NLP. Moore-Lewis cross-entropy difference and follow-ups established perplexity-based domain selection \citep{moore2010cediff,axelrod2015few,axelrod2017cynical}. Recent LLM-scale pruning leverages perplexity or loss from small reference models \citep{ankner2022perplexprune} and surveys consolidate techniques \citep{datasel2024survey}. Active learning routinely employs entropy/margin sampling \citep{zhang2022alsurvey}. EKD is orthogonal: we operate at the token level within sequences and combine selection with unbiased sparse logits.

\subsection{Self-distillation and EMA teachers} Self-distillation regularizes students via a teacher derived from the student itself, e.g., Born-Again Networks \citep{furlanello2018ban} and Mean Teacher (EMA) consistency \citep{tarvainen2017meanteacher}. We adopt an optional EMA self-distill term as a light regularizer complementary to external-teacher KD.

\subsection{Evaluation under shift} We report in-distribution and distribution-shift results following ShiftKD \citep{zhang2023shiftkd}, which benchmarks KD methods under diversity and correlation shifts.

\subsection{Summary} Prior work either improves the \emph{where} (token/word/sample selection, curricula) \emph{or} the \emph{how} (on-policy, reverse-KL, cross-tokenizer, sparse logits) of KD. We unify both: allocate KD budget to high-value tokens and deliver unbiased supervision there via Random-Sampling KD.

% --------------------------
% Suggested BibTeX keys (map to your .bib)
% hinton2015distillation, sanh2019distilbert, kim2016sequencekd, gu2023minillm,
% agarwal2024gkd, boizard2024uld, raman2023slim, anshumann2025sparse,
% shum2024first, guo2017calibration, wang2021selectivekd, liu2025tokenlevel,
% zhang2023continualkd, feng2024kpod, su2023eakd, cheng2024demkd, xu2023unix,
% guo2024entropykd, moore2010cediff, axelrod2015few, axelrod2017cynical,
% ankner2022perplexprune, datasel2024survey, zhang2022alsurvey, furlanello2018ban,
% tarvainen2017meanteacher, zhang2023shiftkd


% --------------------------
% Suggested BibTeX keys (map to your .bib)
% hinton2015distillation, sanh2019distilbert, kim2016sequencekd, gu2023minillm,
% agarwal2024gkd, boizard2024uld, raman2023slim, anshumann2025sparse,
% shum2024first, guo2017calibration, wang2021selectivekd, liu2025tokenlevel,
% feng2024kpod, liu2025srd, furlanello2018ban, tarvainen2017meanteacher, zhang2023shiftkd

\section{Engines}

To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.
\begin{table}
	\centering
	\begin{tabular}{lc}
		\hline
		\textbf{Command} & \textbf{Output} \\
		\hline
		\verb|{\"a}|     & {\"a}           \\
		\verb|{\^e}|     & {\^e}           \\
		\verb|{\`i}|     & {\`i}           \\
		\verb|{\.I}|     & {\.I}           \\
		\verb|{\o}|      & {\o}            \\
		\verb|{\'u}|     & {\'u}           \\
		\verb|{\aa}|     & {\aa}           \\\hline
	\end{tabular}
	\begin{tabular}{lc}
		\hline
		\textbf{Command} & \textbf{Output} \\
		\hline
		\verb|{\c c}|    & {\c c}          \\
		\verb|{\u g}|    & {\u g}          \\
		\verb|{\l}|      & {\l}            \\
		\verb|{\~n}|     & {\~n}           \\
		\verb|{\H o}|    & {\H o}          \\
		\verb|{\v r}|    & {\v r}          \\
		\verb|{\ss}|     & {\ss}           \\
		\hline
	\end{tabular}
	\caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
	\label{tab:accents}
\end{table}
\section{Preamble}
\begin{table*}
	\centering
	\begin{tabular}{lll}
		\hline
		\textbf{Output} & \textbf{natbib command} & \textbf{Old ACL-style command} \\
		\hline
		\hline
	\end{tabular}
	\caption{\label{citation-guide}
		Citation commands supported by the style file.
		The style is based on the natbib package and supports all natbib citation commands.
		It also supports commands defined in previous ACL style files for compatibility.
	}
\end{table*}
The first line of the file must be
\begin{quote}
	\begin{verbatim}
\documentclass[11pt]{article}
\end{verbatim}
\end{quote}
To load the style file in the review version:
\begin{quote}
	\begin{verbatim}
\usepackage[review]{ACL2023}
\end{verbatim}
\end{quote}
For the final version, omit the \verb|review| option:
\begin{quote}
	\begin{verbatim}
\usepackage{ACL2023}
\end{verbatim}
\end{quote}
To use Times Roman, put the following in the preamble:
\begin{quote}
	\begin{verbatim}
\usepackage{times}
\end{verbatim}
\end{quote}
(Alternatives like txfonts or newtx are also acceptable.)
Please see the \LaTeX{} source of this document for comments on other packages that may be useful.
Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.
By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
\begin{quote}
	\begin{verbatim}
\setlength\titlebox{<dim>}
\end{verbatim}
\end{quote}
where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

\section{Document Body}


\subsection{Footnotes}

Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

\subsection{Tables and figures}

See Table~\ref{tab:accents} for an example of a table and its caption.
\textbf{Do not override the default caption sizes.}

\subsection{Hyperlinks}

Users of older versions of \LaTeX{} may encounter the following error during compilation:
\begin{quote}
	\tt\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
\end{quote}
This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
\begin{quote}
	\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{custom}
\end{verbatim}
\end{quote}
You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
To include both the Anthology and your own .bib file, use the following instead of the above.
\begin{quote}
	\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{anthology,custom}
\end{verbatim}
\end{quote}
Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

\subsection{Appendices}

Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

\section{Bib\TeX{} Files}
\label{sec:bibtex}

Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section*{Limitations}
ACL 2023 requires all submissions to have a section titled ``Limitations'', for discussing the limitations of the paper as a complement to the discussion of strengths in the main text. This section should occur after the conclusion, but before the references. It will not count towards the page limit.
The discussion of limitations is mandatory. Papers without a limitation section will be desk-rejected without review.

While we are open to different types of limitations, just mentioning that a set of results have been shown for English only probably does not reflect what we expect.
Mentioning that the method works mostly for languages with limited morphology, like English, is a much better alternative.
In addition, limitations such as low scalability to long text, the requirement of large GPU resources, or other things that inspire crucial further investigation are welcome.

\section*{Ethics Statement}
Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

\section*{Acknowledgements}
This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell,
ACL 2012 by Maggie Li and Michael White,
ACL 2010 by Jing-Shin Chang and Philipp Koehn,
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
ACL 2002 by Eugene Charniak and Dekang Lin,
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is a section in the appendix.

\end{document}
