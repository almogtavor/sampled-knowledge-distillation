% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs} % Added for \toprule, \midrule, and \bottomrule
\usepackage{amsmath} % Added for \tfrac and other math commands
\usepackage{amssymb} % Added for \mathbb and additional math symbols
\usepackage{adjustbox}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{booktabs} % Added for \toprule, \midrule, and \bottomrule


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SampledKD: Extreme Sampling, Full Distillation Accuracy}
% \title{SampledKD: How Extreme Can Sampling Be While Matching Accuracy?}
% \title{SampledKD: How Efficient Could Distillation Get While Match Accuracy?}
% \title{SampledKD: Could Distillation be So Much More Efficient?}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Almog Tavor\textsuperscript{*} \qquad Itay Ebenspanger\textsuperscript{*} \qquad Neil Cnaan\textsuperscript{*} \\
  Balvatnik School of Computer Science and AI \\
  Tel Aviv University \\
  \texttt{\{almogt, ebenspanger, neilcnaan\}@mail.tau.ac.il} \\
}

\begin{document}
\maketitle
\let\thefootnote\relax
\footnotemark
\footnotetext{* Equal contribution.}
\begin{abstract}
	Large language models (LLMs) achieve impressive results but are challenging to deploy due to their size and computational demands. Knowledge distillation (KD) is a popular approach for compressing LLMs, yet existing methods often suffer from efficiency-accuracy trade-offs, especially when applied to autoregressive models. We propose Entropy Knowledge Distillation (EKD), a token-selective distillation framework that leverages teacher entropy and other uncertainty measures to focus supervision on informative tokens, while maintaining unbiased class-level guidance. EKD enables efficient student training by reducing the need to store or query full teacher logits, and improves calibration and performance compared to top-$K$ truncation and other sparse KD methods. Experiments on standard language modeling benchmarks demonstrate that EKD achieves superior compression-accuracy trade-offs, paving the way for more practical deployment of compact LLMs.
\end{abstract}

\section{Introduction}

Large language models (LLMs) deliver state-of-the-art performance but are costly to serve and adapt. Knowledge distillation (KD) amortizes these costs by training a compact student to imitate a larger teacher \citep{hinton2015distillation}. While classic logit-based KD is effective for encoder-style models (e.g., DistilBERT \citep{sanh2019distilbert}), applying KD to autoregressive LLMs raises two persistent obstacles: (i) \textbf{train-inference mismatch} in sequence generation, and (ii) \textbf{efficiency limits} from storing or querying full teacher logits.

To address (i), recent work proposes \emph{on-policy} distillation that trains the student on prefixes it actually produces, with the teacher scoring those prefixes \citep{agarwal2024gkd}. To address (ii), sparse alternatives avoid caching the full distribution. However, deterministic top-$K$ truncation of teacher logits (e.g., keeping only the largest $K$ probabilities) yields biased supervision and degraded calibration, especially for small $K$ \citep{anshumann2025sparse,shum2024first}. Storing more logits improves quality but erodes the efficiency goal.

We revisit \emph{where} and \emph{how} to apply teacher supervision for LLMs. Our thesis is that the KD budget should be spent \emph{selectively at the token level} (where the teacher can add the most information) while keeping \emph{unbiased} class-level supervision when it is applied. Concretely, we:

\begin{itemize}
	\item Introduce \textbf{token-selective distillation}: choose a subset of tokens for KD using scores such as teacher entropy, teacher-student KL, and student CE;
	\item Estimate the teacher distribution at selected tokens with \textbf{Random-Sampling KD}---importance-sampling teacher classes and reweighting---so the gradient matches full KD in expectation while storing only a handful of logits \citep{anshumann2025sparse};
	\item Maintain standard cross-entropy (CE) on \emph{all} tokens to stabilize training and calibration \citep{guo2017calibration}; and
	\item Add simple curricula (increasing KD coverage over time) and optional EMA-based self-distillation for robustness.
\end{itemize}

Empirically, we compare against (a) full Random-Sampling KD at every token, (b) deterministic top-$K$ logit truncation baselines including SLIM-style sparse logits \citep{raman2023slim}, and (c) token/data selection schemes such as GLS for NMT \citep{wang2021selectivekd}, token-level uncertainty-aware post-training \citep{liu2025tokenlevel}, and progressive chain-of-thought distillation \citep{feng2024kpod}. We evaluate both in-distribution and under distribution shift using the ShiftKD protocol \citep{zhang2023shiftkd}. Our results indicate that token-selective, \emph{unbiased} KD attains a stronger accuracy-efficiency Pareto frontier while preserving calibration.

\subsection{Contributions} (1) A unified framework for \emph{token-level} selection with unbiased class-level distillation; (2) practical scoring, curriculum, and bandit variants for allocating KD budget; (3) comprehensive evaluation vs. state-of-the-art sparse KD and selection baselines, including calibration and shift robustness.

\section{Entropy Approximation}
\label{sec:entropy}
We estimate token uncertainty via a Top-$k$-plus-tail construction. Let $u \in \mathbb{R}^V$ be teacher logits over a vocabulary of size $V$, and let
$
	p_i=\frac{e^{u_i}}{Z},\quad Z=\sum_{j=1}^V e^{u_j}
$
denote the softmax probabilities and partition function. Order indices so that $u_{(1)} \geq \cdots \geq u_{(m)}$ are the top-$m$ logits. Define the top-$m$ log-partition and an upper bound on the full log-partition by
\begin{align*}
	Z_m \;           & =\; \log \sum_{j \leq m} e^{u_{(j)}}\,,          \\
	Z_{\text{ub}} \; & =\; \log\Big(e^{Z_m} + (V-m)\,e^{u_{(m)}}\Big).
\end{align*}
i.e., pessimistically setting all tail logits to $u_{(m)}$. For $j\le m$ this yields conservative probabilities:
\[
	p^{\min}_{(j)}=\exp(u_{(j)}-Z_{\text{ub}}).
\]
The remaining probability mass outside the top-$m$ is then bounded by
\begin{align*}
	\tau_{\max}=1-\sum_{j\le m}p^{\min}_{(j)},
\end{align*}
A valid lower bound on entropy $H(p)=-\sum_i p_i\log p_i$ is
\[
	H_{\text{lb}}=-\sum_{j\le m} p^{\min}_{(j)}\log p^{\min}_{(j)}\;-\;\tau_{\max}\log\tau_{\max},
\]
achieved by merging all tail mass into a single bin (coarse-graining; see \citep{cover2006elements} for background and \citep{kaltchenko2025entropyheatmap}, Sec.~3.6, for a self-contained proof and LLM application). A corresponding upper estimate is
\begin{align*}
	H_{\text{ub}}=H_{\text{lb}}+\tau_{\max}\log(V-m),
\end{align*}
(uniform tail), and the midpoint
\begin{align*}
	\widehat{H}_{\text{mid}} & =\tfrac12\!\left(H_{\text{lb}}+H_{\text{ub}}\right)                                   \\
	                         & = -\sum_{j\le m}p^{\min}_{(j)}\log p^{\min}_{(j)} \;-\; \tau_{\max}\log\tau_{\max} \; \\
	                         & \quad +\; \tfrac12\,\tau_{\max}\log(V-m).
\end{align*}

\paragraph{Choice of estimator and $m$.}
We found that for positions ranking, using $\widehat{H}_{\text{mid}}$ with $m{=}12$ serves as the best accuracy--efficiency trade-off (Table~\ref{tab:entropy-ablation}). On \texttt{GSM8K} (7{,}473 sequences; 613K tokens; 438K valid), we ablated $m\in\{8,12,15,20\}$ and compared midpoint to lower bound, upper bound, unbiased importance sampling \emph{(IS)} and control-variated importance sampling \emph{(CV-IS)} tail estimators (both use random sampling from the tail; Appendix~\ref{app:tail-IS}).
Midpoint gives the best top-$k$ position overlap with exact entropy at substantially lower $m$; IS/CV-IS give the best numeric correlation, which is less relevant to the task of position selection. This reduces compute from $\mathcal{O}(V)$ to $\mathcal{O}(m)$: with $V{=}30$K and $m{=}12$, we touch $\sim\!12$ terms (plus one tail) instead of 30K, i.e., $\sim\!2{,}500\times$ fewer.

\vspace{-0.5em}
\begin{table}[h]
	\centering
	\small
	\setlength{\tabcolsep}{6pt}
	\begin{tabular}{rcccccc}
		\toprule
		$m$ & \multicolumn{3}{c}{Avg. Overlap $\uparrow$} & \multicolumn{3}{c}{Avg. Corr. $\uparrow$}                                                                     \\
		\cmidrule(lr){2-4}\cmidrule(lr){5-7}
		    & Mid                                         & IS                                        & CV-IS          & Mid            & IS             & CV-IS          \\
		\midrule
		8   & 0.947                                       & 0.868                                     & 0.867          & 0.966          & 0.981          & 0.981          \\
		12  & \textbf{0.965}                              & 0.918                                     & 0.918          & 0.972          & 0.991          & 0.991          \\
		15  & 0.971                                       & 0.942                                     & 0.944          & 0.975          & 0.993          & 0.993          \\
		20  & 0.976                                       & \textbf{0.956}                            & \textbf{0.957} & \textbf{0.977} & \textbf{0.995} & \textbf{0.995} \\
		\bottomrule
	\end{tabular}
	\caption{GSM8K ablation ($k\%{=}20$ for position selection; Qwen3-0.6B; $s{=}5$ samples for IS/CV-IS). Midpoint attains near-peak overlap already at $m{=}12$; IS/CV-IS maximize correlation at larger $m$. (IS: importance sampling; CV-IS: control-variated IS.)}
	\label{tab:entropy-ablation}
\end{table}
\vspace{-0.75em}


\section{Position-Selective Distillation (RS-KD over Positions)}
\label{sec:posrs}

The KD objective for an autoregressive step is a mean over positions:
\[
	\mathcal{L}_{\text{KD}} \;=\; \frac{1}{N_{\text{pos}}} \sum_{t=1}^{N_{\text{pos}}}
	\mathrm{KL}\!\left(p_t \,\|\, s_t\right),
\]
where $p_t$ and $s_t$ are teacher and student class distributions at position $t$.
Full KD evaluates all positions; we instead \emph{sample positions} using a proposal
$q_{\text{pos}}(t)$ to focus compute on informative steps while keeping the estimate (approximately) unbiased.

\subsection{Sampling rule}
We form $q_{\text{pos}}(t) \propto H_t^\alpha$ from a per-token uncertainty score $H_t$
(e.g., the TkT-LB lower-bound entropy from \S\ref{sec:entropy}, with $m{=}20$) and exponent $\alpha\!\ge\!0$ (higher $\alpha$ concentrates on uncertain tokens). For a sample size $K_{\text{pos}}$, the with-replacement importance-sampling estimator is
\[
	\widehat{\mathcal{L}}_{\text{KD}}^{\text{pos}} \;=\;
	\frac{1}{K_{\text{pos}}} \sum_{t \sim q_{\text{pos}}}
	\frac{1}{q_{\text{pos}}(t)} \,\mathrm{KL}\!\left(p_t \,\|\, s_t\right),
\]
which is unbiased for the uniform positional mean. In practice, we sample \emph{without} replacement within each sequence for lower variance and use a HÃ¡jek-style normalization: $w_t \propto 1/q_{\text{pos}}(t)$, $\sum_{t\in S} w_t=1$, then take $\sum_{t\in S} w_t\,\mathrm{KL}(p_t\|s_t)$.
A tiny probability floor (e.g., $10^{-6}$) stabilizes $1/q$.

\subsection{Discussion}
This \emph{position-selective} KD is orthogonal to RS-KD over classes: it reduces compute along the \emph{time/sequence} axis, not the vocabulary axis. It complements RS-KD (next section) and preserves coverage: low-uncertainty positions still contribute with nonzero probability (controlled by $\alpha$).
We keep CE on all positions for stability, and treat the position-sampled KD as an additive term.

\vspace{0.5em}
\noindent\textbf{Defaults.}
$\alpha{=}1$, $K_{\text{pos}}\!\in\![8,32]$, weight floor $10^{-6}$; optionally anneal $\alpha$ upward during training to shift from exploration to focused supervision.


\section{Two-Axis Offline Sampling with Entropy Proxies (SampledKD)}
\label{sec:twoaxis}

We combine (i) \emph{position-selective} KD (Y axis) with (ii) \emph{RS-KD over classes} (X axis) and (iii) a lightweight \emph{entropy approximation} to build an efficient, unbiased estimator we call \textbf{SampledKD}.
Crucially, we separate \emph{offline} teacher processing from \emph{online} student training.

\subsection{Offline stage (teacher, once)}
For each training sequence and position $t$:
\begin{enumerate}
	\item Compute a top-$m$ entropy proxy $H_t$ via TkT-LB (\S\ref{sec:entropy}) to drive position selection.
	\item Construct a class proposal $q_{\text{voc}}(\cdot\,|\,t)$ (typically $q_{\text{voc}}\!=\!p_t$, or a tempered variant) and draw $R$ samples with replacement. Let $C_t(i)$ be the sampled count of class $i$.
	\item Form a sub-distribution over the sampled classes using importance weights
	      \[
		      \tilde p_t(i) \;\propto\; \frac{p_t(i)}{q_{\text{voc}}(i\,|\,t)} \, C_t(i),
		      \qquad i \in \mathcal{S}_t,
	      \]
	      then normalize over $\mathcal{S}_t=\{i: C_t(i)>0\}$. Choosing $R$ so that $\mathbb{E}[|\mathcal{S}_t|]\!\approx\!K_{\text{voc}}$ yields a budget comparable to top-$K_{\text{voc}}$ but without truncation bias.
\end{enumerate}
Only $(\mathcal{S}_t, \tilde p_t)$ and the scalar $H_t$ are stored (no full logits), typically in fp16.

\subsection{Online stage (student, many epochs)}
At each batch we:
\begin{enumerate}
	\item For each sequence, sample a subset $S$ of positions using $q_{\text{pos}}(t)\!\propto\!H_t^\alpha$ (without replacement, $|S|{=}K_{\text{pos}}$) and weights $w_t \propto 1/q_{\text{pos}}(t)$, $\sum_{t\in S}w_t{=}1$.
	\item For each $t \in S$, compute the \emph{sampled} KD term only on the cached subset:
	      \begin{align*}
		      \mathrm{KL}\!\left(\tilde p_t \,\middle\|\, s_t(\cdot\,;\,\mathcal{S}_t)\right)
		      \; & =\; \sum_{i \in \mathcal{S}_t} \tilde p_t(i)\,\log \tilde p_t(i) \\
		         & \quad - \sum_{i \in \mathcal{S}_t} \tilde p_t(i)\,\log s_t(i).
	      \end{align*}
	\item Aggregate over positions with the positional weights and add CE on all tokens:
	      \begin{align*}
		      \mathcal{L} \; & =\; \sum_{t\in S} w_t\, \mathrm{KL}\!\left(\tilde p_t \,\|\, s_t\right) \\
		                     & \quad + \alpha_{\text{ce}} \cdot \text{CE}(\text{gold}, s).
	      \end{align*}
\end{enumerate}

\subsection{Why it works}
RS-KD's importance weighting ensures that, for fixed $t$, the expectation of the class-sampled KD equals full KD at $t$; positional importance weighting targets the uniform mean over positions.
TkT-LB supplies $H_t$ at $\mathcal{O}(m)$ cost, so both sampling decisions avoid any full-$V$ passes.
Together, SampledKD reduces cost from $\mathcal{O}(N_{\text{pos}}V)$ to $\mathcal{O}(K_{\text{pos}}K_{\text{voc}})$ per sequence while matching full-KD gradients in expectation.

\subsection{Complexity and knobs}
Storage per position is $\mathcal{O}(K_{\text{voc}})$ (indices $+$ probs). Online compute scales with $K_{\text{pos}}\!\times\!K_{\text{voc}}$. Good defaults:
$K_{\text{voc}}\!\approx\!12$ expected uniques (choose $R$ accordingly),
$\tau{=}1$ for $q_{\text{voc}}$ temperature,
$K_{\text{pos}}\!\in\![8,32]$,
$\alpha{=}1$,
$\alpha_{\text{ce}}\!\in\![0.05,0.2]$.
A tiny probability floor prevents numerical blow-ups in $1/q$.

\subsection{Storage accounting}
Our cache packs each stored vocab sample into exactly 3 bytes: a 17-bit token id and a 7-bit probability (q7), written as a single 24-bit word. In addition, we store a per-position entropy approximation $\hat H$ in uint8 (1 byte). With $U{=}12$ stored samples per valid position, the average cost is
\[
	\text{B/token} = U \cdot 3 \;+ \underbrace{1}_{\text{uint8 }\hat H}
	= 12\cdot 3 + 1 = 37\text{B/token},
\]
which yields $3.7$ TB for $N{=}10^{11}{=}100$B tokens. Compared to a vanilla RS-KD cache that keeps only the packed samples, our layout adds only \textbf{1}B for entropy per position:
\[
	\underbrace{(12 \cdot 3)}_{\text{RS-KD }=36} \;+\; \underbrace{1}_{\text{ours}}
	= 37\text{B/token}.
\]

If the selection strategy is fixed and deterministic (e.g., always distill on the top-25\% highest-entropy positions chosen offline), we can achieve substantial additional storage and compute savings by omitting $\hat H$ entirely and only retain RS-KD samples at the selected positions. This slashes storage by the selection rate $k$: with $k{=}0.25$,
\begin{align*}
	\text{B/token} \;&=\; k \cdot U \cdot 3
	\;=\; 0.25 \cdot 12 \cdot 3 \\
	\;&=\; 9~\text{B/token} \ \Longrightarrow\ 0.9~\text{TB}.
\end{align*}
We implemented this optimized layout, but retained the full cache for our ablations so that we can ablate different position-selection policies at training time without rebuilding the cache, but the one-time offline preselection is straightforward and offers an additional $\sim\!\times4$ reduction when $k{=}25\%$.

\begin{table}[h]
	\centering
	\begin{adjustbox}{max width=\linewidth}
		\begin{tabular}{lrrr}
			\toprule
			Method                    & Entries / token          & B / entry & Total (TB)   \\
			\midrule
			Full KD                   & 100{,}000                & 1         & 10{,}000.0   \\
			Top-K 300                 & 300                      & 3         & 90.0         \\
			RS-KD (paper)             & 12                       & 3         & 3.6          \\
			\textbf{Ours (+H=1B)}     & 12                       & 3         & \textbf{3.7} \\
			\quad offline sel.~(25\%) & $12 \!\times\! 0.25 = 3$ & 3         & \textbf{0.9} \\
			Vanilla CE                & 1                        & 3         & 0.3          \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Cache footprint for $N{=}10^{11}{=}100\text{B}$ train tokens. RS-KD (paper) follows their packing: 17-bit vocab ID + 7-bit prob $=$ 3 B per sampled logit, 12 samples/token $\Rightarrow$ 36 B/token (3.6 TB). \textit{Ours} stores the same 12$\times$3 B plus a per-position entropy $\hat H$ in uint8 (additional 1 B/token), totaling 37 B/token (3.7 TB). The \textit{offline sel.~(25\%)} variant assumes deterministic preselection of the top-25\% positions, so $\hat H$ is omitted and storage scales by $k{=}0.25$ to $0.25{\cdot}12{\cdot}3{=}9$ B/token (0.9 TB). Totals use decimal TB, $\mathrm{TB}=\text{bytes/token}\times N/10^{12}$.}
	\label{tab:storage}
\end{table}

\section{Evaluation}
\label{sec:evaluation}

We evaluate SampledKD on a diverse set of language understanding and reasoning benchmarks to assess the effectiveness of our token-selective distillation approach. Models are pretrained on FineWeb-Edu for 1M tokens and evaluated zero-shot. All evaluations report pass@1 accuracy as the primary metric.
\subsection{Datasets}

\paragraph{Mathematical reasoning.}
\textbf{GSM8K} \citep{cobbe2021gsm8k} contains grade-school math word problems with natural language solutions. We use the standard train/test split for distillation training and evaluation.
\textbf{SVAMP} \citep{patel2021svamp} provides single-step arithmetic reasoning problems with algebraic variations. We leverage the available train/test split for consistent evaluation.

\paragraph{Reading comprehension and commonsense reasoning.}
\textbf{LAMBADA (OpenAI)} \citep{paperno2016lambada} tests language models' ability to predict the final word of passages requiring broad context. Since this benchmark provides only a test set without an official training split, we use it exclusively for zero-shot evaluation.
\textbf{HellaSwag} \citep{zellers2019hellaswag} requires selecting the most plausible continuation of everyday scenarios. We use the standard train/validation/test split.
\textbf{PIQA} \citep{bisk2019piqa} evaluates physical commonsense reasoning through questions about everyday situations. Evaluation follows the train/validation/test split.

\paragraph{Science and knowledge.}
\textbf{ARC-Challenge and ARC-Easy} \citep{clark2018arc} test science question answering at middle-school level, with Challenge focusing on questions requiring reasoning and Easy covering direct retrieval. Both use train/dev/test splits.

\paragraph{Competition-level benchmarks.}
\textbf{AIME'25} evaluates mathematical reasoning on problems from the American Invitational Mathematics Examination. 
\textbf{IFEval} \citep{zhou2023ifeval} assesses instruction following through verifiable constraints.

\subsection{Experimental setup}

For benchmarks with available training splits (GSM8K, SVAMP, ARC-Challenge, ARC-Easy, HellaSwag, PIQA), we apply SampledKD during student training and evaluate on the respective test sets. For evaluation-only benchmarks (LAMBADA, AIME'25, IFEval), we assess the distilled models in a zero-shot setting to measure knowledge transfer effectiveness.

All experiments report pass@1 accuracy, measuring the percentage of problems solved correctly on the first attempt without sampling multiple outputs.

\section{Related Work}

Foundational KD \citep{hinton2015distillation} and encoder-model distillation (e.g., DistilBERT \citep{sanh2019distilbert}) have been extended to generative LMs, including sequence-level KD for neural machine translation (NMT) \citep{kim2016sequencekd}, student-to-teacher KL distillation (MiniLLM) \citep{gu2023minillm}, and on-policy distillation, where the student generates its own text and learns from the teacher's feedback, to mitigate exposure bias (GKD) \citep{agarwal2024gkd}.
More recently, cross-tokenizer distillation has been enabled by Universal Logit Distillation (ULD) \citep{boizard2024uld} and others.

\subsection{Sparse logit KD and calibration}
Deterministic top-$K$/percentile caching of teacher logits (e.g., SLIM \citep{raman2023slim} and top-$5$ variants) reduces storage but discards tail mass, inducing biased gradients and miscalibrated students.
Random-Sampling KD \citep{anshumann2025sparse} replaces truncation with importance sampling to provide unbiased gradient estimates and improved calibration.
While exploring orthogonal improvements such as CE--KLD mixing and confidence-based reweighting at the loss level, our work targets token-level selection with different confidence metrics.
Trustworthy distillation (\emph{FIRST}) focuses on calibration by processing top-$k$ vocabulary tokens with temperature scaling and label smoothing \citep{shum2024first}, whereas we apply entropy/KL-based position selection coupled with RS-KD over classes.

\subsection{Token-/word-level selective supervision} Several strands selectively apply supervision at the granularity we target. In NMT, \citet{wang2021selectivekd} select high cross-entropy \emph{words} using both batch-local and global FIFO queues (GLS). For LLM post-training, token-level uncertainty-aware learning applies masked MLE on high-uncertainty \emph{tokens} and self-distillation on the remainder to avoid OOD overfitting \citep{liu2025tokenlevel}. In continual KD, token-level cross-entropy is used to quantify when and where to distill \citep{zhang2023continualkd}. In CoT reasoning, KPOD learns token importance weights and a progressive schedule within rationales \citep{feng2024kpod}. Compared to these, our EKD selects tokens using teacher-centric uncertainty (entropy and teacher-student KL) and couples selection with \emph{unbiased} class-level Random-Sampling KD at selected positions; we also keep CE on all tokens for stability.

\subsection{Entropy/uncertainty-guided KD} Beyond selection, multiple KD variants weight supervision by uncertainty or entropy. Entropy-based adaptive KD (EA-KD) prioritizes \emph{samples} with higher entropy \citep{su2023eakd}, while DE-MKD uses teacher-prediction entropy to weight among multiple teachers \citep{cheng2024demkd}. Uncertainty-aware mixup reduces KD cost while maintaining quality \citep{xu2023unix}. Recent work also shows entropy-weighted distillation can improve reliability/calibration in classification \citep{guo2024entropykd}. Our EKD differs in using entropy/KL primarily to decide \emph{which tokens} receive external-teacher KD, not to reweight losses globally.

% (moved) Entropy approximation subsection relocated before Related Work.

\subsection{Entropy/perplexity for data selection} Selecting \emph{samples} by entropy or perplexity is long-standing in NLP. Moore-Lewis cross-entropy difference and follow-ups established perplexity-based domain selection \citep{moore2010cediff,axelrod2015few,axelrod2017cynical}. Recent LLM-scale pruning leverages perplexity or loss from small reference models \citep{ankner2024perplexedperplexityperplexitybaseddata} and surveys consolidate techniques \citep{datasel2024survey}. Active learning routinely employs entropy/margin sampling \citep{zhang2022alsurvey}. EKD is orthogonal: we operate at the token level within sequences and combine selection with unbiased sparse logits.

\subsection{Self-distillation and EMA teachers} Self-distillation regularizes students via a teacher derived from the student itself, e.g., Born-Again Networks \citep{furlanello2018ban} and Mean Teacher (EMA) consistency \citep{tarvainen2017meanteacher}. We adopt an optional EMA self-distill term as a light regularizer complementary to external-teacher KD.

\subsection{Evaluation under shift} We report in-distribution and distribution-shift results following ShiftKD \citep{zhang2023shiftkd}, which benchmarks KD methods under diversity and correlation shifts.

\subsection{Summary} Prior work either improves the \emph{where} (token/word/sample selection, curricula) or the \emph{how} (on-policy, reverse-KL, cross-tokenizer, sparse logits) of KD. We unify both: allocate KD budget to high-value tokens and deliver unbiased supervision there via Random-Sampling KD.

\bibliography{anthology, custom}
\bibliographystyle{acl_natbib}

\appendix
\section{Alternative tail estimators (unbiased IS and control variates)}
\label{app:tail-IS}
Let $T$ be the tail indices (outside top-$m$), $P_T=\sum_{v\in T}p_v$, and $q(v)=p_v/P_T$ the tail-normalized sampling distribution. The exact tail \emph{contribution to entropy} is
\[
	H_T \;=\; -\sum_{v\in T} p_v \log p_v \;=\; -P_T\,\underset{v\sim q}{\mathbb{E}}\,[\log p_v].
\]
\textbf{Top-$m$ + naive IS.} With $s$ samples $v_i\!\sim q$,
\[
	\widehat{H}_T^{\text{IS}}
	= -P_T \cdot \frac{1}{s}\sum_{i=1}^s \log p_{v_i}.
\]
\textbf{Top-$m$ + control-variated IS (uniform baseline).}
Decompose with the uniform tail $u=\tfrac{P_T}{V-m}$:
\begin{align*}
	\sum_{v\in T} p_v \log p_v
	&= -P_T\log\!\frac{P_T}{V-m} \\
	&+ P_T\,\underset{v\sim q}{\mathbb{E}}\!\left[\log\!\frac{p_v}{u}\right].
\end{align*}
Thus, the entropy tail is
\[
	H_T^{\text{CV-IS}}
	= \underbrace{-P_T\log\!\frac{P_T}{V-m}}_{\text{baseline}}
	\;-\; P_T \cdot \underset{v\sim q}{\mathbb{E}}\!\left[\log\!\frac{p_v}{u}\right],
\]
with unbiased estimator (using $s$ samples):
\begin{align*}
	\widehat{H}_T^{\text{CV-IS}}
	 & = -P_T\log\!\frac{P_T}{V-m}                                                 \\
	 & \quad - P_T \cdot \frac{1}{s}\sum_{i=1}^s \Big(\log p_{v_i} - \log u\Big).
\end{align*}
In all cases, the head contribution $H_{\text{head}}=-\sum_{j\le m}p_{(j)}\log p_{(j)}$ is computed exactly; the final estimate is $H=H_{\text{head}}+\widehat{H}_T$.

\end{document}
