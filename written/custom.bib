% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% Knowledge Distillation Papers

@article{hinton2015distillation,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015},
  url={https://arxiv.org/abs/1503.02531}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019},
  url={https://arxiv.org/abs/1910.01108}
}

@article{agarwal2024gkd,
  title={On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
  author={Agarwal, Rishabh and Vieillard, Nino and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
  journal={arXiv preprint arXiv:2306.13649},
  year={2024},
  url={https://arxiv.org/abs/2306.13649}
}

@article{anshumann2025sparse,
  title={Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs},
  author={Anshumann, Piyush and Agarwal, Rishabh and Bachem, Olivier},
  journal={arXiv preprint arXiv:2503.16870},
  year={2025},
  url={https://arxiv.org/abs/2503.16870}
}

@article{wang2021selectivekd,
  title={Selective Knowledge Distillation for Neural Machine Translation},
  author={Wang, Fusheng and Xu, Jianhao and Zhao, Fandong and Jia, Jianfeng and Zhou, Jie},
  journal={arXiv preprint arXiv:2105.12967},
  year={2021},
  url={https://arxiv.org/abs/2105.12967}
}

@article{zhang2023shiftkd,
  title={Benchmarking Knowledge Distillation under Distribution Shift},
  author={Zhang, Jinghan and Agarwal, Rishabh and Babuschkin, Igor and Bachem, Olivier},
  journal={arXiv preprint arXiv:2312.16242},
  year={2023},
  url={https://arxiv.org/abs/2312.16242}
}

@article{feng2024kpod,
  title={Keypoint-based Progressive Chain-of-Thought Distillation for LLMs},
  author={Feng, Kaituo and Zhang, Changsheng and Wu, Ye and Zhang, Guoren},
  journal={arXiv preprint arXiv:2405.16064},
  year={2024},
  url={https://arxiv.org/abs/2405.16064}
}

@article{liu2025tokenlevel,
  title={Token-Level Uncertainty-Aware Objective for Language Model Post-Training},
  author={Liu, Jiahao and Chen, Tongxu and Zhang, Chengming and Liu, Hongwei and Wang, Haochen and Peng, Nenghai},
  journal={arXiv preprint arXiv:2503.16511},
  year={2025},
  url={https://arxiv.org/abs/2503.16511}
}

@article{guo2017calibration,
  title={On Calibration of Modern Neural Networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  journal={arXiv preprint arXiv:1706.04599},
  year={2017},
  url={https://arxiv.org/abs/1706.04599}
}

@inproceedings{furlanello2018ban,
  title={Born-Again Neural Networks},
  author={Furlanello, Tommaso and Lipton, Zachary C. and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle={Proceedings of Machine Learning Research},
  volume={80},
  pages={1607--1616},
  year={2018},
  url={https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf}
}

@article{boizard2024uld,
  title={Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs},
  author={Boizard, Nicolas and Colombo, Pierre and Piantanida, Pablo and Clavel, Chlo{\'e}},
  journal={arXiv preprint arXiv:2402.12030},
  year={2024},
  url={https://arxiv.org/abs/2402.12030}
}

@inproceedings{raman2023slim,
  title={For Distillation, Tokens Are Not All You Need},
  author={Raman, Neeraj and Vare, Siddharth and Srinivasan, Apurva and Chandra, Vignesh and Khandelwal, Kush},
  booktitle={OpenReview},
  year={2023},
  url={https://openreview.net/pdf?id=2fc5GOPYip}
}

@article{shum2024first,
  title={On the Calibration and Trustworthiness of Knowledge Distillation},
  author={Shum, Hubert and Wang, Linlin and Chen, Jiajun and Zheng, Liang},
  journal={arXiv preprint},
  year={2024}
}

@article{kim2016sequencekd,
  title={Sequence-Level Knowledge Distillation},
  author={Kim, Yoon and Rush, Alexander M.},
  journal={arXiv preprint arXiv:1606.07947},
  year={2016},
  url={https://arxiv.org/abs/1606.07947}
}

@article{gu2023minillm,
  title={Knowledge Distillation of Large Language Models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal={arXiv preprint arXiv:2306.08543},
  year={2023},
  url={https://arxiv.org/abs/2306.08543}
}

@article{tarvainen2017meanteacher,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{liu2025srd,
  title={Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning},
  author={Liu, Ming and Qi, Jie and Wang, Yicheng and Han, Jiawei and Chen, Lei},
  journal={arXiv preprint},
  year={2025}
}
