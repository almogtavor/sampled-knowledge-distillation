% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% Knowledge Distillation & Selection (LLMs / NLP)

@article{hinton2015distillation,
  title   = {Distilling the Knowledge in a Neural Network},
  author  = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal = {arXiv preprint arXiv:1503.02531},
  year    = {2015},
  url     = {https://arxiv.org/abs/1503.02531}
}

@article{sanh2019distilbert,
  title   = {Distil{BERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author  = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal = {arXiv preprint arXiv:1910.01108},
  year    = {2019},
  url     = {https://arxiv.org/abs/1910.01108}
}

@article{agarwal2024gkd,
  title   = {On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
  author  = {Agarwal, Rishabh and Vieillard, Nino and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
  journal = {arXiv preprint arXiv:2306.13649},
  year    = {2024},
  url     = {https://arxiv.org/abs/2306.13649}
}

@article{anshumann2025sparse,
  title   = {Sparse Logit Sampling: Accelerating Knowledge Distillation in {LLM}s},
  author  = {Anshumann, A. and Zaidi, Mohd Abbas and Kedia, Akhil and Ahn, Jinwoo and Kwon, Taehwak and Lee, Kangwook and Lee, Haejun and Lee, Joohyung},
  journal = {arXiv preprint arXiv:2503.16870},
  year    = {2025},
  url     = {https://arxiv.org/abs/2503.16870}
}

@article{wang2021selectivekd,
  title   = {Selective Knowledge Distillation for Neural Machine Translation},
  author  = {Wang, Fusheng and Yan, Jianhao and Meng, Fandong and Zhou, Jie},
  journal = {arXiv preprint arXiv:2105.12967},
  year    = {2021},
  url     = {https://arxiv.org/abs/2105.12967}
}

@article{zhang2023shiftkd,
  title   = {Benchmarking Knowledge Distillation under Distribution Shift},
  author  = {Zhang, Jinghan and Agarwal, Rishabh and Babuschkin, Igor and Bachem, Olivier},
  journal = {arXiv preprint arXiv:2312.16242},
  year    = {2023},
  url     = {https://arxiv.org/abs/2312.16242}
}

@article{feng2024kpod,
  title   = {Keypoint-based Progressive Chain-of-Thought Distillation for {LLM}s},
  author  = {Feng, Kaituo and Li, Changsheng and Zhang, Xiaolu and Zhou, Jun and Yuan, Ye and Wang, Guoren},
  journal = {arXiv preprint arXiv:2405.16064},
  year    = {2024},
  url     = {https://arxiv.org/abs/2405.16064}
}

@article{liu2025tokenlevel,
  title   = {Token-Level Uncertainty-Aware Objective for Language Model Post-Training},
  author  = {Liu, Tingkai and Benjamin, Ari S. and Zador, Anthony M.},
  journal = {arXiv preprint arXiv:2503.16511},
  year    = {2025},
  url     = {https://arxiv.org/abs/2503.16511}
}

@inproceedings{guo2017calibration,
  title     = {On Calibration of Modern Neural Networks},
  author    = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {1321--1330},
  year      = {2017},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v70/guo17a.html}
}

@inproceedings{furlanello2018ban,
  title     = {Born-Again Neural Networks},
  author    = {Furlanello, Tommaso and Lipton, Zachary C. and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {1607--1616},
  year      = {2018},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf}
}

@article{boizard2024uld,
  title   = {Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for {LLM}s},
  author  = {Boizard, Nicolas and El Haddad, Kevin and Hudelot, C{\'e}line and Colombo, Pierre},
  journal = {arXiv preprint arXiv:2402.12030},
  year    = {2024},
  url     = {https://arxiv.org/abs/2402.12030},
  doi     = {10.48550/arXiv.2402.12030}
}

@misc{raman2023slim,
  title        = {For Distillation, Tokens Are Not All You Need},
  author       = {Raman, Neeraj and Vare, Siddharth and Srinivasan, Apurva and Chandra, Vignesh and Khandelwal, Kush},
  howpublished = {OpenReview},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=2fc5GOPYip}
}

@inproceedings{shum2024first,
  title     = {{FIRST}: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation},
  author    = {Shum, Kashun and Xu, Minrui and Zhang, Jianshu and Chen, Zixin and Diao, Shizhe and Dong, Hanze and Zhang, Jipeng and Raza, Muhammad Omer},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {12646--12659},
  year      = {2024},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.emnlp-main.703.pdf}
}

@article{kim2016sequencekd,
  title   = {Sequence-Level Knowledge Distillation},
  author  = {Kim, Yoon and Rush, Alexander M.},
  journal = {arXiv preprint arXiv:1606.07947},
  year    = {2016},
  url     = {https://arxiv.org/abs/1606.07947}
}

@article{gu2023minillm,
  title   = {Mini{LLM}: Knowledge Distillation of Large Language Models},
  author  = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal = {arXiv preprint arXiv:2306.08543},
  year    = {2023},
  url     = {https://arxiv.org/abs/2306.08543}
}

@inproceedings{tarvainen2017meanteacher,
  title     = {Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results},
  author    = {Tarvainen, Antti and Valpola, Harri},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {30},
  year      = {2017}
}

@article{liu2025srd,
  title   = {Selective Reflection-Tuning: Student-Selected Data Recycling for {LLM} Instruction-Tuning},
  author  = {Li, Ming and Chen, Lichang and Chen, Jiuhai and He, Shwai and Gu, Jiuxiang and Zhou, Tianyi},
  journal = {arXiv preprint arXiv:2402.10110},
  year    = {2024},
  url     = {https://arxiv.org/abs/2402.10110},
  note    = {ACL 2024 version available on OpenReview}
}

@article{zhang2023continualkd,
  title   = {Continual Knowledge Distillation for Neural Machine Translation},
  author  = {Zhang, Yuanchi and Li, Peng and Sun, Maosong and Liu, Yang},
  journal = {arXiv preprint arXiv:2212.09097},
  year    = {2022},
  url     = {https://arxiv.org/abs/2212.09097}
}

@article{su2023eakd,
  title   = {{EA}-{KD}: Entropy-based Adaptive Knowledge Distillation},
  author  = {Su, Cheng-Wei and Tseng, Shih-Hsin and Martins, Jo{\~a}o Vitor and Ichimura, Naoyuki and Seiji, Yokou and Chou, Chien-Hsing},
  journal = {arXiv preprint arXiv:2311.13621},
  year    = {2023},
  url     = {https://arxiv.org/abs/2311.13621}
}

@article{cheng2024demkd,
  title     = {{DE}-{MKD}: Decoupled Multi-Teacher Knowledge Distillation Based on Entropy},
  author    = {Cheng, Xiaojie and Zhang, Zhenqi and Weng, Wenzheng and Yu, Weiqiang and Zhou, Jun},
  journal   = {Mathematics},
  volume    = {12},
  number    = {11},
  pages     = {1672},
  year      = {2024},
  publisher = {MDPI},
  url       = {https://www.mdpi.com/2227-7390/12/11/1672}
}

@article{xu2023unix,
  title   = {Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup},
  author  = {Xu, Guodong and Liu, Ziwei and Loy, Chen Change},
  journal = {Pattern Recognition},
  volume  = {138},
  pages   = {109338},
  year    = {2023},
  doi     = {10.1016/j.patcog.2023.109338},
  url     = {https://arxiv.org/abs/2012.09413}
}

@article{guo2024entropykd,
  title   = {Leveraging Logit Uncertainty for Better Knowledge Distillation},
  author  = {Guo, Zhen and Wang, Dong and He, Qiang and Zhang, Pengzhou},
  journal = {Scientific Reports},
  volume  = {14},
  number  = {31249},
  year    = {2024},
  doi     = {10.1038/s41598-024-82647-6},
  url     = {https://www.nature.com/articles/s41598-024-82647-6}
}

@inproceedings{moore2010cediff,
  title     = {Intelligent Selection of Language Model Training Data},
  author    = {Moore, Robert C. and Lewis, William},
  booktitle = {Proceedings of the ACL 2010 Conference Short Papers},
  pages     = {220--224},
  year      = {2010},
  address   = {Uppsala, Sweden},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P10-2041/}
}

@inproceedings{axelrod2015few,
  title     = {Class-based N-gram Language Difference Models for Data Selection},
  author    = {Axelrod, Amittai and Vyas, Yogarshi and Martindale, Marianna J. and Carpuat, Marine},
  booktitle = {Proceedings of IWSLT 2015},
  year      = {2015},
  url       = {https://workshop2015.iwslt.org/downloads/Axelrod-2015.12.03-IWSLT.pdf}
}

@article{axelrod2017cynical,
  title   = {Cynical Selection of Language Model Training Data},
  author  = {Axelrod, Amittai},
  journal = {arXiv preprint arXiv:1709.02279},
  year    = {2017},
  url     = {https://arxiv.org/abs/1709.02279}
}

@article{ankner2024perplexprune,
  title   = {Perplexity-Pruned Data: Can Smaller Language Models Do More With Less?},
  author  = {Ankner, Gabriel and Hewitt, John and Liu, Nelson F.},
  journal = {arXiv preprint arXiv:2405.20541},
  year    = {2024},
  url     = {https://arxiv.org/abs/2405.20541}
}

@article{datasel2024survey,
  title   = {A Survey on Data Selection for Language Models},
  author  = {Albalak, Alon and Elazar, Yanai and Xie, Sang Michael and Longpre, Shayne and Lambert, Nathan and Wang, Xinyi and Muennighoff, Niklas and Hou, Bairu and Pan, Liangming and Jeong, Haewon and Raffel, Colin and Chang, Shiyu and Hashimoto, Tatsunori and Wang, William Yang},
  journal = {arXiv preprint arXiv:2402.16827},
  year    = {2024},
  url     = {https://arxiv.org/abs/2402.16827}
}

@inproceedings{zhang2022alsurvey,
  title     = {A Survey of Active Learning for Natural Language Processing},
  author    = {Zhang, Zhisong and Strubell, Emma and Hovy, Eduard},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages     = {6166--6190},
  year      = {2022},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.414.pdf}
}

@article{kaltchenko2025entropyheatmap,
  title   = {Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis},
  author  = {Kaltchenko, Alexei},
  journal = {arXiv preprint arXiv:2505.00746},
  year    = {2025},
  url     = {https://arxiv.org/abs/2505.00746},
  doi     = {10.48550/arXiv.2505.00746}
}

@book{cover2006elements,
  title     = {Elements of Information Theory},
  author    = {Cover, Thomas M. and Thomas, Joy A.},
  year      = {2006},
  edition   = {2nd},
  publisher = {Wiley-Interscience},
  address   = {Hoboken, NJ, USA},
  isbn      = {978-0-471-24195-9}
}

@misc{ankner2024perplexedperplexityperplexitybaseddata,
  title         = {Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models},
  author        = {Zachary Ankner and Cody Blakeney and Kartik Sreenivasan and Max Marion and Matthew L. Leavitt and Mansheej Paul},
  year          = {2024},
  eprint        = {2405.20541},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2405.20541}
}

@article{cobbe2021gsm8k,
  title   = {Training Verifiers to Solve Math Word Problems},
  author  = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mark and Chen, Jacob and Jun, Heewoo and Kaiser, Łukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John and Kaplan, Jared and McCandlish, Sam and Amodei, Dario},
  journal = {arXiv preprint arXiv:2110.14168},
  year    = {2021},
  url     = {https://arxiv.org/abs/2110.14168}
}

@inproceedings{patel2021svamp,
  title     = {Are NLP Models really able to Solve Simple Math Word Problems?},
  author    = {Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year      = {2021},
  url       = {https://aclanthology.org/2021.naacl-main.168}
}

@inproceedings{paperno2016lambada,
  title     = {The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author    = {Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2016},
  url       = {https://aclanthology.org/P16-1144}
}

@inproceedings{zellers2019hellaswag,
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  author    = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2019},
  url       = {https://aclanthology.org/P19-1472}
}

@misc{bisk2019piqa,
  title         = {PIQA: Reasoning about Physical Commonsense in Natural Language},
  author        = {Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
  year          = {2019},
  eprint        = {1911.11641},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1911.11641}
}

@inproceedings{clark2018arc,
  title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author    = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2018},
  url       = {https://aclanthology.org/P18-1260}
}

@article{zhou2023ifeval,
  title   = {Instruction-Following Evaluation for Large Language Models},
  author  = {Zhou, Shuyan and Schick, Timo and Xu, Haoyang and Roberts, Adam and Artetxe, Mikel and Zettlemoyer, Luke and Smith, Noah A. and Lewis, Mike},
  journal = {arXiv preprint arXiv:2307.11763},
  year    = {2023},
  url     = {https://arxiv.org/abs/2307.11763}
}
