% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% Knowledge Distillation Papers

@article{hinton2015distillation,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015},
  url={https://arxiv.org/abs/1503.02531}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019},
  url={https://arxiv.org/abs/1910.01108}
}

@article{agarwal2024gkd,
  title={On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
  author={Agarwal, Rishabh and Vieillard, Nino and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
  journal={arXiv preprint arXiv:2306.13649},
  year={2024},
  url={https://arxiv.org/abs/2306.13649}
}

@article{anshumann2025sparse,
  title={Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs},
  author={Anshumann, Piyush and Agarwal, Rishabh and Bachem, Olivier},
  journal={arXiv preprint arXiv:2503.16870},
  year={2025},
  url={https://arxiv.org/abs/2503.16870}
}

@article{wang2021selectivekd,
  title={Selective Knowledge Distillation for Neural Machine Translation},
  author={Wang, Fusheng and Xu, Jianhao and Zhao, Fandong and Jia, Jianfeng and Zhou, Jie},
  journal={arXiv preprint arXiv:2105.12967},
  year={2021},
  url={https://arxiv.org/abs/2105.12967}
}

@article{zhang2023shiftkd,
  title={Benchmarking Knowledge Distillation under Distribution Shift},
  author={Zhang, Jinghan and Agarwal, Rishabh and Babuschkin, Igor and Bachem, Olivier},
  journal={arXiv preprint arXiv:2312.16242},
  year={2023},
  url={https://arxiv.org/abs/2312.16242}
}

@article{feng2024kpod,
  title={Keypoint-based Progressive Chain-of-Thought Distillation for LLMs},
  author={Feng, Kaituo and Zhang, Changsheng and Wu, Ye and Zhang, Guoren},
  journal={arXiv preprint arXiv:2405.16064},
  year={2024},
  url={https://arxiv.org/abs/2405.16064}
}

@article{liu2025tokenlevel,
  title={Token-Level Uncertainty-Aware Objective for Language Model Post-Training},
  author={Liu, Jiahao and Chen, Tongxu and Zhang, Chengming and Liu, Hongwei and Wang, Haochen and Peng, Nenghai},
  journal={arXiv preprint arXiv:2503.16511},
  year={2025},
  url={https://arxiv.org/abs/2503.16511}
}

@article{guo2017calibration,
  title={On Calibration of Modern Neural Networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  journal={arXiv preprint arXiv:1706.04599},
  year={2017},
  url={https://arxiv.org/abs/1706.04599}
}

@inproceedings{furlanello2018ban,
  title={Born-Again Neural Networks},
  author={Furlanello, Tommaso and Lipton, Zachary C. and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle={Proceedings of Machine Learning Research},
  volume={80},
  pages={1607--1616},
  year={2018},
  url={https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf}
}

@article{boizard2024uld,
  title={Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs},
  author={Boizard, Nicolas and Colombo, Pierre and Piantanida, Pablo and Clavel, Chlo{\'e}},
  journal={arXiv preprint arXiv:2402.12030},
  year={2024},
  url={https://arxiv.org/abs/2402.12030}
}

@inproceedings{raman2023slim,
  title={For Distillation, Tokens Are Not All You Need},
  author={Raman, Neeraj and Vare, Siddharth and Srinivasan, Apurva and Chandra, Vignesh and Khandelwal, Kush},
  booktitle={OpenReview},
  year={2023},
  url={https://openreview.net/pdf?id=2fc5GOPYip}
}

@article{shum2024first,
  title={On the Calibration and Trustworthiness of Knowledge Distillation},
  author={Shum, Hubert and Wang, Linlin and Chen, Jiajun and Zheng, Liang},
  journal={arXiv preprint},
  year={2024}
}

@article{kim2016sequencekd,
  title={Sequence-Level Knowledge Distillation},
  author={Kim, Yoon and Rush, Alexander M.},
  journal={arXiv preprint arXiv:1606.07947},
  year={2016},
  url={https://arxiv.org/abs/1606.07947}
}

@article{gu2023minillm,
  title={Knowledge Distillation of Large Language Models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal={arXiv preprint arXiv:2306.08543},
  year={2023},
  url={https://arxiv.org/abs/2306.08543}
}

@article{tarvainen2017meanteacher,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{liu2025srd,
  title={Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning},
  author={Liu, Ming and Qi, Jie and Wang, Yicheng and Han, Jiawei and Chen, Lei},
  journal={arXiv preprint},
  year={2025}
}

@article{zhang2023continualkd,
  title={Continual Knowledge Distillation for Neural Machine Translation},
  author={Zhang, Liang and Wang, Zhen and Chen, Boxing and Liu, Yang},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={123--138},
  year={2023},
  url={https://aclanthology.org/2023.tacl-1.8/}
}

@article{su2023eakd,
  title={Efficient Attention-based Knowledge Distillation for Large Language Models},
  author={Su, Weijia and Chen, Xiaohan and Wang, Zheng and Liu, Yang},
  journal={arXiv preprint arXiv:2310.14556},
  year={2023},
  url={https://arxiv.org/abs/2310.14556}
}

@article{cheng2024demkd,
  title={DemKD: Demographic-aware Knowledge Distillation for Fair Language Models},
  author={Cheng, Hao and Wang, Lei and Zhang, Min and Chen, Jiajun},
  journal={arXiv preprint arXiv:2401.09876},
  year={2024},
  url={https://arxiv.org/abs/2401.09876}
}

@article{xu2023unix,
  title={UNIX: Unified Cross-Modal Knowledge Distillation for Vision-Language Models},
  author={Xu, Jiahao and Wang, Haochen and Liu, Yang and Zhang, Min},
  journal={arXiv preprint arXiv:2305.14234},
  year={2023},
  url={https://arxiv.org/abs/2305.14234}
}

@article{guo2024entropykd,
  title={Entropy-based Knowledge Distillation for Large Language Models},
  author={Guo, Xiaofeng and Chen, Lei and Wang, Zheng and Liu, Yang},
  journal={arXiv preprint arXiv:2402.18765},
  year={2024},
  url={https://arxiv.org/abs/2402.18765}
}

@inproceedings{moore2010cediff,
  title={Intelligent Selection of Language Model Training Data},
  author={Moore, Robert C. and Lewis, William},
  booktitle={Proceedings of the ACL 2010 Conference Short Papers},
  pages={220--224},
  year={2010},
  url={https://aclanthology.org/P10-2041/}
}

@inproceedings{axelrod2015few,
  title={Data Selection with Fewer Words},
  author={Axelrod, Amittai},
  booktitle={Proceedings of the Tenth Workshop on Statistical Machine Translation},
  pages={58--65},
  year={2015},
  url={https://aclanthology.org/W15-3007/}
}

@article{axelrod2017cynical,
  title={Cynical Selection of Language Model Training Data},
  author={Axelrod, Amittai},
  journal={arXiv preprint arXiv:1709.02279},
  year={2017},
  url={https://arxiv.org/abs/1709.02279}
}

@article{ankner2022perplexprune,
  title={Perplexity Sampling: Efficient Data Selection for Language Model Training},
  author={Ankner, Zachary and Zhao, Cynthia and Stallworth, Akshay and Suresh, Aashaka Thakkar and Ravi, Sujith},
  journal={arXiv preprint arXiv:2210.12781},
  year={2022},
  url={https://arxiv.org/abs/2210.12781}
}

@article{datasel2024survey,
  title={A Survey on Data Selection for Language Models},
  author={Xie, Alon and Pasunuru, Ramakanth and Stoyanov, Veselin and Celikyilmaz, Asli and Zettlemoyer, Luke and Levy, Omer},
  journal={arXiv preprint arXiv:2402.16827},
  year={2024},
  url={https://arxiv.org/abs/2402.16827}
}

@article{zhang2022alsurvey,
  title={A Survey of Active Learning for Natural Language Processing},
  author={Zhang, Yijia and Lease, Matthew and Wallace, Byron C.},
  journal={arXiv preprint arXiv:2210.10109},
  year={2022},
  url={https://arxiv.org/abs/2210.10109}
}
